{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb63f26",
   "metadata": {},
   "source": [
    "# LoRA Edge Vision - End-to-End Demo\n",
    "\n",
    "This script demonstrates the full workflow of LoRA fine-tuning of Stable Diffusion for \n",
    "aerial imagery, optimized for edge deployment with ONNX.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project showcases:\n",
    "- Low-Rank Adaptation (LoRA) fine-tuning of Stable Diffusion for aerial/satellite imagery\n",
    "-  Memory-efficient training techniques\n",
    "- Exporting models to ONNX format for edge deployment\n",
    "- Performance comparison between PyTorch and ONNX inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea004854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import onnxruntime as ort\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Set paths to our models\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "LORA_PATH = \"models/lora_adapters\"\n",
    "MERGED_MODEL_PATH = \"models/sd_lora_pipeline\"\n",
    "ONNX_DIR = \"onnx_mac\"\n",
    "OUTPUT_DIR = \"demo_outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b785048",
   "metadata": {},
   "source": [
    "### 1. Project Architecture\n",
    "\n",
    "The overall architecture of the project follows these steps:\n",
    "1. Precompute VAE latents from aerial imagery\n",
    "2. Train LoRA adapter on latents\n",
    "3. Merge LoRA adapter with base model\n",
    "4. Export to ONNX for edge deployment\n",
    "5. Run inference on edge devices\n",
    "\n",
    "### 2. Load and Test PyTorch Model with LoRA\n",
    "\n",
    "First, we load our LoRA-adapted model in PyTorch and generate sample images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pytorch_model(base_model, lora_adapter, device=\"cpu\"):\n",
    "  \"\"\"Load PyTorch model with LoRA adapter\"\"\"\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float32\n",
    "  ).to(device)\n",
    "\n",
    "  pipe.enable_attention_slicing()\n",
    "  pipe.unet = PeftModel.from_pretrained(pipe.unet, lora_adapter)\n",
    "\n",
    "  return pipe\n",
    "\n",
    "def generate_pytorch(pipe, prompt, steps=30, scale=7.5, seed=None):\n",
    "  \"\"\"Generate image using PyTorch model\"\"\"\n",
    "  generator = None\n",
    "  if seed is not None:\n",
    "    generator = torch.Generator(device=pipe.device).manual_seed(seed)\n",
    "\n",
    "  start_time = time.time()\n",
    "  image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=steps,\n",
    "    guidance_scale=scale,\n",
    "    generator=generator\n",
    "  ).images[0]\n",
    "  end_time = time.time()\n",
    "\n",
    "  return image, end_time - start_time\n",
    "\n",
    "# Load the PyTorch model with LoRA adapter\n",
    "print(\"Loading PyTorch model with LoRA adapter...\")\n",
    "pytorch_pipe = load_pytorch_model(BASE_MODEL, LORA_PATH, DEVICE)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test prompt ideas\n",
    "test_prompts = [\n",
    "  \"Aerial view of agricultural fields with irrigation patterns\",\n",
    "  \"Satellite imagery of coastal urban area with beaches\",\n",
    "  \"Bird's eye view of mountain landscape with snow caps\",\n",
    "  \"Top-down view of desert patterns with sand dunes\",\n",
    "  \"Aerial photograph of river delta with sediment patterns\"\n",
    "]\n",
    "\n",
    "# Generate sample image with PyTorch model\n",
    "print(\"Generating sample image with PyTorch model...\")\n",
    "seed = 42  # For reproducibility\n",
    "sample_prompt = test_prompts[0]\n",
    "pytorch_image, pytorch_time = generate_pytorch(pytorch_pipe, sample_prompt, steps=30, seed=seed)\n",
    "\n",
    "# Save the image\n",
    "pytorch_path = os.path.join(OUTPUT_DIR, \"pytorch_sample.png\")\n",
    "pytorch_image.save(pytorch_path)\n",
    "print(f\"PyTorch generation took {pytorch_time:.2f} seconds\")\n",
    "print(f\"Sample image saved to {pytorch_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fc6eb",
   "metadata": {},
   "source": [
    "### 3. Load and Test ONNX Model\n",
    "\n",
    "Now, we test the ONNX version of our model, which is optimized for edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_onnx_components(onnx_dir, base_model):\n",
    "  \"\"\"Load ONNX UNet and other components\"\"\"\n",
    "  from diffusers import DDIMScheduler\n",
    "\n",
    "  # Set up ONNX Runtime session\n",
    "  sess_options = ort.SessionOptions()\n",
    "  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "  # Load UNet ONNX model\n",
    "  unet_path = os.path.join(onnx_dir, \"unet.onnx\")\n",
    "  unet_session = ort.InferenceSession(unet_path, sess_options)\n",
    "\n",
    "  # Load other components from PyTorch\n",
    "  sd_pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float32)\n",
    "\n",
    "  return {\n",
    "    \"unet\": unet_session,\n",
    "    \"vae\": sd_pipe.vae,\n",
    "    \"text_encoder\": sd_pipe.text_encoder,\n",
    "    \"tokenizer\": sd_pipe.tokenizer,\n",
    "    \"scheduler\": DDIMScheduler.from_pretrained(base_model, subfolder=\"scheduler\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52714fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onnx(components, prompt, steps=30, scale=7.5, seed=None):\n",
    "  \"\"\"Generate image using ONNX model\"\"\"\n",
    "  import torch.nn.functional as F\n",
    "\n",
    "  # Set random seed for reproducibility\n",
    "  if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "  unet = components[\"unet\"]\n",
    "  vae = components[\"vae\"]\n",
    "  text_encoder = components[\"text_encoder\"]\n",
    "  tokenizer = components[\"tokenizer\"]\n",
    "  scheduler = components[\"scheduler\"]\n",
    "\n",
    "  # Get UNet input and output names\n",
    "  unet_inputs = unet.get_inputs()\n",
    "  unet_output = unet.get_outputs()[0].name\n",
    "\n",
    "  # Process text input\n",
    "  text_input = tokenizer(\n",
    "    [prompt],\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids)[0].numpy()\n",
    "\n",
    "  # Unconditional embeddings for guidance\n",
    "  uncond_input = tokenizer(\n",
    "    [\"\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids)[0].numpy()\n",
    "\n",
    "  # Concatenate for classifier-free guidance\n",
    "  text_embeddings = np.concatenate([uncond_embeddings, text_embeddings])\n",
    "\n",
    "  # Setup scheduler\n",
    "  scheduler.set_timesteps(steps)\n",
    "\n",
    "  # Create random latents\n",
    "  latents_shape = (1, 4, 64, 64)  # Standard for 512x512 images\n",
    "  latents = np.random.randn(*latents_shape).astype(np.float32)\n",
    "  latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Denoising loop\n",
    "  for i, t in enumerate(scheduler.timesteps):\n",
    "    # Duplicate latents for classifier-free guidance\n",
    "    latent_model_input = np.repeat(latents, 2, axis=0)\n",
    "\n",
    "    # Get timestep\n",
    "    timestep = np.array([t]).astype(np.int64)\n",
    "\n",
    "    # Get input names based on exported model\n",
    "    onnx_inputs = {}\n",
    "    for input_info in unet_inputs:\n",
    "        if \"sample\" in input_info.name.lower():\n",
    "            onnx_inputs[input_info.name] = latent_model_input\n",
    "        elif \"timestep\" in input_info.name.lower():\n",
    "            onnx_inputs[input_info.name] = timestep\n",
    "        elif \"encoder_hidden_states\" in input_info.name.lower():\n",
    "            onnx_inputs[input_info.name] = text_embeddings\n",
    "\n",
    "    # Run inference\n",
    "    noise_pred = unet.run([unet_output], onnx_inputs)[0]\n",
    "\n",
    "    # Perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = np.split(noise_pred, 2)\n",
    "    noise_pred = noise_pred_uncond + scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Compute previous noisy sample\n",
    "    latents = scheduler.step(\n",
    "        torch.from_numpy(noise_pred),\n",
    "        t,\n",
    "        torch.from_numpy(latents)\n",
    "    ).prev_sample.numpy()\n",
    "\n",
    "  # Decode latents\n",
    "  latents = 1 / 0.18215 * latents\n",
    "\n",
    "  with torch.no_grad():\n",
    "    latents_torch = torch.from_numpy(latents)\n",
    "    image = vae.decode(latents_torch).sample\n",
    "\n",
    "  # Convert to image\n",
    "  image = (image / 2 + 0.5).clamp(0, 1)\n",
    "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "  image = (image * 255).round().astype(\"uint8\")[0]\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  return Image.fromarray(image), end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX components\n",
    "print(\"Loading ONNX model components...\")\n",
    "onnx_components = load_onnx_components(ONNX_DIR, BASE_MODEL)\n",
    "print(\"ONNX components loaded successfully!\")\n",
    "\n",
    "# Generate sample image with ONNX model\n",
    "print(\"Generating sample image with ONNX model...\")\n",
    "onnx_image, onnx_time = generate_onnx(onnx_components, sample_prompt, steps=30, seed=seed)\n",
    "\n",
    "# Save and display the image\n",
    "onnx_path = os.path.join(OUTPUT_DIR, \"onnx_sample.png\")\n",
    "onnx_image.save(onnx_path)\n",
    "print(f\"ONNX generation took {onnx_time:.2f} seconds\")\n",
    "print(f\"ONNX sample saved to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd57dd8",
   "metadata": {},
   "source": [
    "### 4. Performance Comparison\n",
    "\n",
    "Let's compare the inference speed of PyTorch vs. ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccde362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(models, prompts, steps=30, trials=3):\n",
    "  \"\"\"Benchmark PyTorch vs ONNX performance\"\"\"\n",
    "  results = {\"pytorch\": [], \"onnx\": []}\n",
    "\n",
    "  for prompt in prompts:\n",
    "    print(f\"Benchmarking prompt: {prompt}\")\n",
    "\n",
    "    # PyTorch benchmark\n",
    "    pytorch_times = []\n",
    "    for i in range(trials):\n",
    "      _, time_taken = generate_pytorch(models[\"pytorch\"], prompt, steps=steps, seed=None)\n",
    "      pytorch_times.append(time_taken)\n",
    "      print(f\"  PyTorch trial {i+1}: {time_taken:.2f}s\")\n",
    "\n",
    "    # ONNX benchmark\n",
    "    onnx_times = []\n",
    "    for i in range(trials):\n",
    "      _, time_taken = generate_onnx(models[\"onnx\"], prompt, steps=steps, seed=None)\n",
    "      onnx_times.append(time_taken)\n",
    "      print(f\"  ONNX trial {i+1}: {time_taken:.2f}s\")\n",
    "\n",
    "    # Record average times\n",
    "    results[\"pytorch\"].append(sum(pytorch_times) / len(pytorch_times))\n",
    "    results[\"onnx\"].append(sum(onnx_times) / len(onnx_times))\n",
    "\n",
    "  return results, prompts\n",
    "\n",
    "# For a quick benchmark, let's select two prompts\n",
    "benchmark_prompts = test_prompts[:2]\n",
    "\n",
    "# Create models dictionary\n",
    "models = {\n",
    "  \"pytorch\": pytorch_pipe,\n",
    "  \"onnx\": onnx_components\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Running performance benchmark (this may take a while)...\")\n",
    "benchmark_results, benchmark_prompts = run_benchmark(models, benchmark_prompts, steps=20, trials=2)\n",
    "\n",
    "# Visualize benchmark results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(benchmark_prompts))\n",
    "width = 0.35\n",
    "\n",
    "pytorch_times = benchmark_results[\"pytorch\"]\n",
    "onnx_times = benchmark_results[\"onnx\"]\n",
    "\n",
    "plt.bar(x - width/2, pytorch_times, width, label='PyTorch')\n",
    "plt.bar(x + width/2, onnx_times, width, label='ONNX')\n",
    "\n",
    "plt.ylabel('Inference Time (seconds)')\n",
    "plt.title('PyTorch vs ONNX Inference Performance')\n",
    "plt.xticks(x, [f\"Prompt {i+1}\" for i in range(len(benchmark_prompts))])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add speedup percentage labels\n",
    "for i, (pt, ox) in enumerate(zip(pytorch_times, onnx_times)):\n",
    "  speedup = (pt - ox) / pt * 100\n",
    "  plt.annotate(f\"{speedup:.1f}% faster\",\n",
    "              xy=(i + width/2, ox),\n",
    "              xytext=(0, 10),\n",
    "              textcoords=\"offset points\",\n",
    "              ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"benchmark.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447861c6",
   "metadata": {},
   "source": [
    "### 5. Visual Comparison\n",
    "\n",
    "Let's compare the visual quality of PyTorch vs. ONNX generations to ensure the edge-optimized model maintains quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0231e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generations(pytorch_pipe, onnx_components, prompts, steps=30, seed=42):\n",
    "  \"\"\"Generate and compare images from PyTorch and ONNX models\"\"\"\n",
    "  plt.figure(figsize=(15, 5 * len(prompts)))\n",
    "\n",
    "  for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating comparison for: {prompt}\")\n",
    "\n",
    "    # Generate with PyTorch\n",
    "    torch_img, torch_time = generate_pytorch(pytorch_pipe, prompt, steps=steps, seed=seed)\n",
    "\n",
    "    # Generate with ONNX\n",
    "    onnx_img, onnx_time = generate_onnx(onnx_components, prompt, steps=steps, seed=seed)\n",
    "\n",
    "    # Plot results\n",
    "    plt.subplot(len(prompts), 2, i*2 + 1)\n",
    "    plt.imshow(np.array(torch_img))\n",
    "    plt.title(f\"PyTorch: {torch_time:.2f}s\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(len(prompts), 2, i*2 + 2)\n",
    "    plt.imshow(np.array(onnx_img))\n",
    "    plt.title(f\"ONNX: {onnx_time:.2f}s\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save individual images\n",
    "    torch_img.save(os.path.join(OUTPUT_DIR, f\"pytorch_sample_{i}.png\"))\n",
    "    onnx_img.save(os.path.join(OUTPUT_DIR, f\"onnx_sample_{i}.png\"))\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(os.path.join(OUTPUT_DIR, \"visual_comparison.png\"))\n",
    "  plt.show()\n",
    "\n",
    "# Compare generations for selected prompts\n",
    "compare_prompts = test_prompts[2:4]  # Use a couple different prompts\n",
    "compare_generations(pytorch_pipe, onnx_components, compare_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7835b",
   "metadata": {},
   "source": [
    "### 6. Memory Usage\n",
    "Let's also compare the memory usage of PyTorch vs. ONNX models, which is critical for edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model_path):\n",
    "  \"\"\"Get size of model files in MB\"\"\"\n",
    "  if os.path.isfile(model_path):\n",
    "    return os.path.getsize(model_path) / (1024 * 1024)\n",
    "\n",
    "  total_size = 0\n",
    "  for dirpath, dirnames, filenames in os.walk(model_path):\n",
    "    for f in filenames:\n",
    "      fp = os.path.join(dirpath, f)\n",
    "      total_size += os.path.getsize(fp)\n",
    "\n",
    "  return total_size / (1024 * 1024)\n",
    "\n",
    "# Get model sizes\n",
    "pytorch_size = get_model_size(MERGED_MODEL_PATH)\n",
    "onnx_size = get_model_size(ONNX_DIR)\n",
    "\n",
    "print(f\"PyTorch model size: {pytorch_size:.2f} MB\")\n",
    "print(f\"ONNX model size: {onnx_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(pytorch_size - onnx_size) / pytorch_size * 100:.2f}%\")\n",
    "\n",
    "# Visualize model sizes\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([\"PyTorch\", \"ONNX\"], [pytorch_size, onnx_size])\n",
    "plt.ylabel(\"Model Size (MB)\")\n",
    "plt.title(\"Model Size Comparison\")\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"size_comparison.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983cbab",
   "metadata": {},
   "source": [
    "### 7. Gallery of Generated Images\n",
    "\n",
    "Let's create a gallery of images generated with our LoRA-tuned model to showcase its capabilities for aerial imagery. This demonstrates how our specialization for aerial/satellite photography has enhanced the model's ability to generate this specific type of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gallery(model, prompts, steps=30):\n",
    "  \"\"\"Generate a gallery of images\"\"\"\n",
    "  gallery = []\n",
    "\n",
    "  for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating image {i+1}/{len(prompts)}: {prompt}\")\n",
    "\n",
    "    # Set different seed for each image for variety\n",
    "    seed = 1000 + i\n",
    "\n",
    "    # Generate with ONNX for speed\n",
    "    img, _ = generate_onnx(model, prompt, steps=steps, seed=seed)\n",
    "\n",
    "    # Save the image\n",
    "    img_path = os.path.join(OUTPUT_DIR, f\"gallery_{i}.png\")\n",
    "    img.save(img_path)\n",
    "\n",
    "    gallery.append((img, prompt))\n",
    "\n",
    "  return gallery\n",
    "\n",
    "# Generate gallery\n",
    "gallery_prompts = [\n",
    "  \"Aerial view of mountain ranges with lakes\",\n",
    "  \"Satellite imagery of tropical island with coral reefs\",\n",
    "  \"Bird's eye view of urban park with winding paths\",\n",
    "  \"Top-down view of agricultural terraces in mountains\",\n",
    "  \"Aerial photograph of meandering river through forest\"\n",
    "]\n",
    "\n",
    "gallery = generate_gallery(onnx_components, gallery_prompts)\n",
    "\n",
    "# Display gallery\n",
    "rows = len(gallery) // 2 + len(gallery) % 2\n",
    "plt.figure(figsize=(15, 5 * rows))\n",
    "\n",
    "for i, (img, prompt) in enumerate(gallery):\n",
    "  plt.subplot(rows, 2, i+1)\n",
    "  plt.imshow(np.array(img))\n",
    "  plt.title(f\"Prompt: {prompt}\", fontsize=10)\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bea6b4",
   "metadata": {},
   "source": [
    "### 8. Conclusion and Key Findings\n",
    "\n",
    "#### Our project demonstrates the successful implementation of:\n",
    "\n",
    "1. **Efficient Fine-tuning**: Using LoRA to adapt Stable Diffusion for aerial imagery with minimal training resources\n",
    "2. **Edge Optimization**: Converting the model to ONNX format for efficient deployment on edge devices\n",
    "3. **Performance Improvements**: \n",
    "  - Faster inference time with ONNX Runtime\n",
    "  - Reduced memory footprint\n",
    "  - Maintained visual quality\n",
    "\n",
    "This approach is particularly valuable for specialized aerial imagery applications that need to run efficiently on edge devices with limited computing resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
